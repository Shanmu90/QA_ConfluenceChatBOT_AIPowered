High-Level Architecture: Confluence ‚Üí RAG QA Chatbot
User Question
     ‚Üì
Frontend Chat UI
     ‚Üì
Backend API (Python / Node)
     ‚Üì
Retriever (Vector DB Search)
     ‚Üë
Embeddings of Confluence Pages
     ‚Üë
Ingestion Pipeline (Confluence ‚Üí Chunking ‚Üí Embeddings)
     ‚Üì
LLM (Answer Generation with Context)
     ‚Üì
Response + Source Citation

STEP 1: Confluence Data Ingestion
‚úÖ What You Can Use from Confluence
You can safely use non-sensitive project documentation, for example:
‚Ä¢	Test Plans
‚Ä¢	Test Strategy documents
‚Ä¢	RTM (Requirement Traceability Matrix)
‚Ä¢	User Stories (functional, not confidential customer data)
‚Ä¢	Regression scope documents
‚Ä¢	Defect RCA (without PII)
‚Ä¢	Release notes
‚Ä¢	FAQ / Known Issues pages
üëâ Avoid: credentials, customer data, production logs with PII
How to Extract Pages
Use Confluence REST API
You will pull:
‚Ä¢	Page Title
‚Ä¢	Page Content (HTML or storage format)
‚Ä¢	Labels (optional, good for filtering like UAT, Payments, Regression)
‚Ä¢	Last updated date (useful for freshness)
Flow:
1.	Authenticate using API token
2.	Fetch pages from selected Spaces (example: QA, TEST, BANKING_UAT)
3.	Store raw content locally as JSON
STEP 2: Preprocessing & Chunking
LLMs cannot take full pages, so you split them.
What You Do:
‚Ä¢	Remove HTML tags
‚Ä¢	Clean headers, tables into readable text
‚Ä¢	Split into chunks of ~300‚Äì500 words
‚Ä¢	Add metadata:

{
  "page_title": "Payments UAT Test Plan",
  "section": "Negative Scenarios",
  "source_url": "confluence link",
  "last_updated": "2025-01-10"
}
This metadata is VERY important for QA traceability.
üîπ STEP 3: Create Embeddings
Now convert each chunk into a vector.
You can use:
‚Ä¢	OpenAI Embeddings API
‚Ä¢	Or open-source models (like Instructor / BGE)
Each chunk ‚Üí embedding ‚Üí stored in vector DB.
________________________________________
üîπ STEP 4: Vector Database (Retriever Layer)
Good options:
‚Ä¢	FAISS (local, simple)
‚Ä¢	Chroma
‚Ä¢	MongoDB Atlas Vector Search (great if you already know MongoDB)
Stored record:
{
  embedding: [vector numbers],
  text_chunk: "...steps to validate failed payment...",
  metadata: {
     page_title: "...",
     source_url: "...",
     labels: ["Payments", "UAT"]
  }
}
STEP 5: Retrieval Flow (When User Asks a Question)
User asks:
‚ÄúWhat are the negative test scenarios for payment timeout?‚Äù
System does:
1.	Convert question ‚Üí embedding
2.	Search vector DB ‚Üí top 3‚Äì5 relevant chunks
3.	Pass to LLM like this:
Prompt to LLM
Answer the question ONLY using the context below.
If the answer is not present, say you don‚Äôt know.

Context:
[Chunk 1 from Confluence]
[Chunk 2 from Confluence]
[Chunk 3 from Confluence]

Question: What are the negative test scenarios for payment timeout?
STEP 6: LLM Answer Generation
LLM (OpenAI / Azure OpenAI) generates:
‚úî Answer based on Confluence
‚úî Not hallucinating from internet
‚úî Grounded in QA documentation
STEP 7: Add Source Citation (Very Important for QA)
Return response like:
Answer:
‚ÄúThe negative scenarios include session timeout during OTP, network failure before confirmation, and delayed callback from payment gateway.‚Äù
Sources:
‚Ä¢	Payments_UAT_TestPlan ‚Üí Negative Scenarios Section
‚Ä¢	Defect_RCA_PaymentTimeout_v2
This builds trust + auditability.
STEP 8: Evaluation Layer (Your QA Superpower)
You don‚Äôt just build ‚Äî you test the AI.
You measure:
Metric	What It Checks
Faithfulness	Is answer supported by retrieved text?
Context Relevance	Did retriever fetch correct chunks?
Answer Relevance	Did LLM actually answer the question?
Hallucination Rate	% answers not backed by source
You can use DeepEval here.

